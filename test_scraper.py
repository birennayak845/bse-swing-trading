#!/usr/bin/env python3
"""
Test script for Web Scraper
Demonstrates scraping from multiple sources and generating recommendations
"""

import sys
import os

# Add parent directory to path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from web_scraper import WebScraper
import pandas as pd
from datetime import datetime

def test_web_scraper():
    """Test the web scraper with multiple stocks"""
    
    print("\n" + "="*80)
    print("üï∑Ô∏è  BSE WEB SCRAPER - MULTI-SOURCE ANALYSIS TEST")
    print("="*80 + "\n")
    
    scraper = WebScraper()
    
    # Test stocks
    test_symbols = ['RELIANCE', 'TCS', 'HDFCBANK']
    
    print(f"üìç Testing {len(test_symbols)} stocks: {', '.join(test_symbols)}\n")
    print("Data sources to be scraped:")
    print("  1. Moneycontrol")
    print("  2. Economic Times")
    print("  3. NSE India Website")
    print("  4. BSE India Official")
    print("  5. TradingView\n")
    
    print("‚îÄ" * 80 + "\n")
    
    # Test single stock scraping from all sources
    print("STEP 1: Scrape single stock from multiple sources\n")
    
    symbol = 'RELIANCE'
    print(f"Scraping {symbol} from all available sources...\n")
    
    sources_to_try = [
        ('Moneycontrol', scraper.scrape_moneycontrol),
        ('Economic Times', scraper.scrape_economictimes),
        ('NSE Website', scraper.scrape_nseindia_table),
        ('BSE India', scraper.scrape_bseindia),
        ('TradingView', scraper.scrape_trading_view)
    ]
    
    successful_sources = []
    
    for source_name, scraper_func in sources_to_try:
        try:
            print(f"  Trying {source_name}...", end=" ")
            result = scraper_func(symbol)
            if result:
                print(f"‚úì SUCCESS - ‚Çπ{result['current_price']:.2f}")
                successful_sources.append((source_name, result['current_price']))
            else:
                print("‚úó No data found")
        except Exception as e:
            print(f"‚úó Error: {str(e)[:50]}")
    
    print(f"\n‚úì Successfully scraped from {len(successful_sources)} sources\n")
    
    if successful_sources:
        print("Prices collected:")
        for source, price in successful_sources:
            print(f"  ‚Ä¢ {source}: ‚Çπ{price:.2f}")
        
        avg_price = sum(p for _, p in successful_sources) / len(successful_sources)
        print(f"\n  Average price: ‚Çπ{avg_price:.2f}")
    
    print("\n" + "‚îÄ" * 80 + "\n")
    
    # Test full analysis pipeline
    print("STEP 2: Full analysis with recommendations\n")
    
    results = scraper.analyze_multiple_stocks(test_symbols, include_historical=False)
    
    print(f"‚úì Analyzed {len(results)} stocks\n")
    
    for result in results:
        print(f"\nüìä {result['symbol']}")
        print(f"   Current Price: ‚Çπ{result['current_price']:.2f}")
        print(f"   Recommendation: {result['recommendation']}")
        print(f"   Confidence: {result['confidence']:.1f}%")
        
        if result['analysis']:
            print(f"   Technical Indicators:")
            for key, value in result['analysis'].items():
                print(f"     ‚Ä¢ {key}: {value:.2f}")
        
        if result['reasoning']:
            print(f"   Analysis:")
            for reason in result['reasoning']:
                print(f"     ‚Ä¢ {reason}")
    
    print("\n" + "="*80)
    print("‚úÖ Web Scraper Test Complete")
    print("="*80 + "\n")

def test_individual_scraper():
    """Test individual scraper function"""
    
    print("\n" + "="*80)
    print("üîç INDIVIDUAL SCRAPER TEST")
    print("="*80 + "\n")
    
    scraper = WebScraper()
    
    symbol = 'RELIANCE'
    
    print(f"Testing all-sources scraper for {symbol}...\n")
    
    result = scraper.scrape_all_sources(symbol)
    
    if result:
        print(f"‚úì Successfully scraped {symbol}")
        print(f"\nData:")
        for key, value in result.items():
            print(f"  {key}: {value}")
    else:
        print(f"‚úó Failed to scrape {symbol}")
    
    print("\n" + "="*80 + "\n")

if __name__ == "__main__":
    # Test both scenarios
    try:
        test_web_scraper()
        test_individual_scraper()
    except KeyboardInterrupt:
        print("\n\n‚èπ  Test interrupted by user")
    except Exception as e:
        print(f"\n\n‚ùå Test failed: {str(e)}")
        import traceback
        traceback.print_exc()
